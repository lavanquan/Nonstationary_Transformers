{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/qula0496/quan/Nonstationary_Transformers/ns_models'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/qula0496/quan/Nonstationary_Transformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qula0496/quan/.venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "id_gpu = '0'\n",
    "n_users = 10\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = id_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_utils import *\n",
    "from utils.model_utils import *\n",
    "# from utils.koopman_utils import *\n",
    "from modules.serverbase import *\n",
    "from modules.userbase import *\n",
    "from modules.servernsTransformer import *\n",
    "from modules.usernsTranformer import *\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.cuda.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['start', 'feat_static_cat', 'feat_dynamic_real', 'item_id', 'target'],\n",
       "        num_rows: 210\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['start', 'feat_static_cat', 'feat_dynamic_real', 'item_id', 'target'],\n",
       "        num_rows: 210\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['start', 'feat_static_cat', 'feat_dynamic_real', 'item_id', 'target'],\n",
       "        num_rows: 210\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"LeoTungAnh/kdd210_hourly\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10898"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset[0]['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_4085075/2019493983.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['date'] = '2015-01-01 00:00:00'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>T3</th>\n",
       "      <th>T4</th>\n",
       "      <th>T5</th>\n",
       "      <th>T6</th>\n",
       "      <th>T7</th>\n",
       "      <th>T8</th>\n",
       "      <th>T9</th>\n",
       "      <th>T10</th>\n",
       "      <th>...</th>\n",
       "      <th>T201</th>\n",
       "      <th>T202</th>\n",
       "      <th>T203</th>\n",
       "      <th>T204</th>\n",
       "      <th>T205</th>\n",
       "      <th>T206</th>\n",
       "      <th>T207</th>\n",
       "      <th>T208</th>\n",
       "      <th>T209</th>\n",
       "      <th>T210</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01 00:00:00</th>\n",
       "      <td>1.468122</td>\n",
       "      <td>1.538769</td>\n",
       "      <td>-0.030617</td>\n",
       "      <td>-0.781501</td>\n",
       "      <td>-0.802695</td>\n",
       "      <td>-0.772417</td>\n",
       "      <td>-0.378809</td>\n",
       "      <td>-0.237514</td>\n",
       "      <td>-0.393948</td>\n",
       "      <td>-0.810264</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.131542</td>\n",
       "      <td>-0.785538</td>\n",
       "      <td>-0.807741</td>\n",
       "      <td>-0.777464</td>\n",
       "      <td>1.493353</td>\n",
       "      <td>1.654833</td>\n",
       "      <td>-0.025571</td>\n",
       "      <td>-0.776959</td>\n",
       "      <td>-0.777464</td>\n",
       "      <td>-0.792602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 00:00:00</th>\n",
       "      <td>1.316855</td>\n",
       "      <td>1.449373</td>\n",
       "      <td>-0.079674</td>\n",
       "      <td>-0.773862</td>\n",
       "      <td>-0.798327</td>\n",
       "      <td>-0.767746</td>\n",
       "      <td>-0.512904</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.507808</td>\n",
       "      <td>-0.803933</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.140836</td>\n",
       "      <td>-0.775391</td>\n",
       "      <td>-0.798327</td>\n",
       "      <td>-0.772842</td>\n",
       "      <td>1.444276</td>\n",
       "      <td>1.724601</td>\n",
       "      <td>-0.023609</td>\n",
       "      <td>-0.804443</td>\n",
       "      <td>-0.767746</td>\n",
       "      <td>-0.783036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 00:00:00</th>\n",
       "      <td>1.261700</td>\n",
       "      <td>1.636722</td>\n",
       "      <td>-0.061296</td>\n",
       "      <td>-0.759776</td>\n",
       "      <td>-0.780089</td>\n",
       "      <td>-0.754046</td>\n",
       "      <td>-0.639456</td>\n",
       "      <td>-0.582161</td>\n",
       "      <td>-0.597787</td>\n",
       "      <td>-0.792069</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.134217</td>\n",
       "      <td>-0.759776</td>\n",
       "      <td>-0.780089</td>\n",
       "      <td>-0.754046</td>\n",
       "      <td>1.501297</td>\n",
       "      <td>1.772147</td>\n",
       "      <td>-0.009210</td>\n",
       "      <td>-0.791548</td>\n",
       "      <td>-0.748838</td>\n",
       "      <td>-0.769672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 00:00:00</th>\n",
       "      <td>1.487590</td>\n",
       "      <td>1.831615</td>\n",
       "      <td>-0.022891</td>\n",
       "      <td>-0.730291</td>\n",
       "      <td>-0.753942</td>\n",
       "      <td>-0.721690</td>\n",
       "      <td>-0.646435</td>\n",
       "      <td>-0.560429</td>\n",
       "      <td>-0.555053</td>\n",
       "      <td>-0.765768</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.114272</td>\n",
       "      <td>-0.733516</td>\n",
       "      <td>-0.753942</td>\n",
       "      <td>-0.732441</td>\n",
       "      <td>0.525398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.270158</td>\n",
       "      <td>-0.744267</td>\n",
       "      <td>-0.678687</td>\n",
       "      <td>-0.748567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 00:00:00</th>\n",
       "      <td>1.842409</td>\n",
       "      <td>2.225288</td>\n",
       "      <td>0.150884</td>\n",
       "      <td>-0.702879</td>\n",
       "      <td>-0.723452</td>\n",
       "      <td>-0.694879</td>\n",
       "      <td>-0.632018</td>\n",
       "      <td>-0.500582</td>\n",
       "      <td>-0.454865</td>\n",
       "      <td>-0.741738</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066271</td>\n",
       "      <td>-0.707451</td>\n",
       "      <td>-0.729166</td>\n",
       "      <td>-0.717737</td>\n",
       "      <td>-0.672020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.569157</td>\n",
       "      <td>-0.743453</td>\n",
       "      <td>-0.557728</td>\n",
       "      <td>-0.723452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  210 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           T1        T2        T3        T4        T5  \\\n",
       "date                                                                    \n",
       "2015-01-01 00:00:00  1.468122  1.538769 -0.030617 -0.781501 -0.802695   \n",
       "2015-01-01 00:00:00  1.316855  1.449373 -0.079674 -0.773862 -0.798327   \n",
       "2015-01-01 00:00:00  1.261700  1.636722 -0.061296 -0.759776 -0.780089   \n",
       "2015-01-01 00:00:00  1.487590  1.831615 -0.022891 -0.730291 -0.753942   \n",
       "2015-01-01 00:00:00  1.842409  2.225288  0.150884 -0.702879 -0.723452   \n",
       "\n",
       "                           T6        T7        T8        T9       T10  ...  \\\n",
       "date                                                                   ...   \n",
       "2015-01-01 00:00:00 -0.772417 -0.378809 -0.237514 -0.393948 -0.810264  ...   \n",
       "2015-01-01 00:00:00 -0.767746 -0.512904  0.000000 -0.507808 -0.803933  ...   \n",
       "2015-01-01 00:00:00 -0.754046 -0.639456 -0.582161 -0.597787 -0.792069  ...   \n",
       "2015-01-01 00:00:00 -0.721690 -0.646435 -0.560429 -0.555053 -0.765768  ...   \n",
       "2015-01-01 00:00:00 -0.694879 -0.632018 -0.500582 -0.454865 -0.741738  ...   \n",
       "\n",
       "                         T201      T202      T203      T204      T205  \\\n",
       "date                                                                    \n",
       "2015-01-01 00:00:00 -0.131542 -0.785538 -0.807741 -0.777464  1.493353   \n",
       "2015-01-01 00:00:00 -0.140836 -0.775391 -0.798327 -0.772842  1.444276   \n",
       "2015-01-01 00:00:00 -0.134217 -0.759776 -0.780089 -0.754046  1.501297   \n",
       "2015-01-01 00:00:00 -0.114272 -0.733516 -0.753942 -0.732441  0.525398   \n",
       "2015-01-01 00:00:00 -0.066271 -0.707451 -0.729166 -0.717737 -0.672020   \n",
       "\n",
       "                         T206      T207      T208      T209      T210  \n",
       "date                                                                   \n",
       "2015-01-01 00:00:00  1.654833 -0.025571 -0.776959 -0.777464 -0.792602  \n",
       "2015-01-01 00:00:00  1.724601 -0.023609 -0.804443 -0.767746 -0.783036  \n",
       "2015-01-01 00:00:00  1.772147 -0.009210 -0.791548 -0.748838 -0.769672  \n",
       "2015-01-01 00:00:00  0.000000 -0.270158 -0.744267 -0.678687 -0.748567  \n",
       "2015-01-01 00:00:00  0.000000 -0.569157 -0.743453 -0.557728 -0.723452  \n",
       "\n",
       "[5 rows x 210 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a list of dictionaries, where each dictionary represents a row in the DataFrame\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for row in test_dataset:\n",
    "  df[row['item_id']] = row['target']\n",
    "\n",
    "df['date'] = '2015-01-01 00:00:00'\n",
    "df = df.set_index(\"date\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the DataFrame to a CSV file\n",
    "# df.to_csv('/home/qula0496/quan/Nonstationary_Transformers/dataset/kdd/kdd.csv', index_label='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10898, 210)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kdd.csv'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ns_models.ns_TransformerConfig import NS_TransformerConfig\n",
    "user_num_ts = int(df.shape[1] / n_users)\n",
    "args = NS_TransformerConfig()\n",
    "args.devices = id_gpu\n",
    "args.enc_in = user_num_ts\n",
    "args.dec_in = user_num_ts\n",
    "args.c_out = user_num_ts\n",
    "args.root_path = './dataset/kdd/'\n",
    "args.data_path = 'kdd.csv'\n",
    "args.model_id = 'kdd_96_96'\n",
    "args.data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from data_provider.data_factory import *\n",
    "from data_provider.data_loader import *\n",
    "\n",
    "test_data, test_loader = data_provider(args, flag='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data_list = []\n",
    "server_data_list = []\n",
    "for i in range(n_users):\n",
    "    train_data, train_loader = data_provider(args, flag='train', start=i*user_num_ts+1, end=min(user_num_ts*(i+1)+1, df.shape[1]+1))\n",
    "    test_data, test_loader = data_provider(args, flag='test', start=i*user_num_ts+1, end=min(user_num_ts*(i+1)+1, df.shape[1]+1))\n",
    "    # user_data = train_set.filter(lambda e, idx: idx>=(i*user_num_ts) and idx < user_num_ts*(i+1), with_indices=True)\n",
    "    user_data_list.append(train_loader)\n",
    "    server_data_list.append(test_loader)\n",
    "    # print(train_data[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (enc_embedding): DataEmbedding(\n",
       "    (value_embedding): TokenEmbedding(\n",
       "      (tokenConv): Conv1d(21, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)\n",
       "    )\n",
       "    (position_embedding): PositionalEmbedding()\n",
       "    (temporal_embedding): TimeFeatureEmbedding(\n",
       "      (embed): Linear(in_features=4, out_features=64, bias=False)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (dec_embedding): DataEmbedding(\n",
       "    (value_embedding): TokenEmbedding(\n",
       "      (tokenConv): Conv1d(21, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)\n",
       "    )\n",
       "    (position_embedding): PositionalEmbedding()\n",
       "    (temporal_embedding): TimeFeatureEmbedding(\n",
       "      (embed): Linear(in_features=4, out_features=64, bias=False)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): Encoder(\n",
       "    (attn_layers): ModuleList(\n",
       "      (0-5): 6 x EncoderLayer(\n",
       "        (attention): AttentionLayer(\n",
       "          (inner_attention): DSAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (key_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (value_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (out_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (conv1): Conv1d(64, 32, kernel_size=(1,), stride=(1,))\n",
       "        (conv2): Conv1d(32, 64, kernel_size=(1,), stride=(1,))\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x DecoderLayer(\n",
       "        (self_attention): AttentionLayer(\n",
       "          (inner_attention): DSAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (key_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (value_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (out_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (cross_attention): AttentionLayer(\n",
       "          (inner_attention): DSAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (key_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (value_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (out_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (conv1): Conv1d(64, 32, kernel_size=(1,), stride=(1,))\n",
       "        (conv2): Conv1d(32, 64, kernel_size=(1,), stride=(1,))\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (projection): Linear(in_features=64, out_features=21, bias=True)\n",
       "  )\n",
       "  (tau_learner): Projector(\n",
       "    (series_conv): Conv1d(96, 1, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)\n",
       "    (backbone): Sequential(\n",
       "      (0): Linear(in_features=42, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=1, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (delta_learner): Projector(\n",
       "    (series_conv): Conv1d(96, 1, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)\n",
       "    (backbone): Sequential(\n",
       "      (0): Linear(in_features=42, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=96, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ns_models import ns_Transformer\n",
    "server_model = ns_Transformer.Model(configs=args)\n",
    "server_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkenlvq\u001b[0m (\u001b[33mquanla\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6314571f60004ceea90b00ae1968c75d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112167665527927, max=1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/qula0496/quan/Nonstationary_Transformers/wandb/run-20240611_143824-3ulg9jy5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/quanla/Federated%20Non-Stationary%20Transformer%20on%20KDD%20dataset/runs/3ulg9jy5' target=\"_blank\">Federated Non-Stationary Transformer on KDD dataset</a></strong> to <a href='https://wandb.ai/quanla/Federated%20Non-Stationary%20Transformer%20on%20KDD%20dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/quanla/Federated%20Non-Stationary%20Transformer%20on%20KDD%20dataset' target=\"_blank\">https://wandb.ai/quanla/Federated%20Non-Stationary%20Transformer%20on%20KDD%20dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/quanla/Federated%20Non-Stationary%20Transformer%20on%20KDD%20dataset/runs/3ulg9jy5' target=\"_blank\">https://wandb.ai/quanla/Federated%20Non-Stationary%20Transformer%20on%20KDD%20dataset/runs/3ulg9jy5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "import torch\n",
    "\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "LR = 1e-3\n",
    "GLOBAL_EPOCHS = 30\n",
    "LOCAL_EPOCHS = 5\n",
    "BATCH_SIZE = 128\n",
    "L2_PENALTY = 0.0\n",
    "USER_RATIO = 0.1\n",
    "run = wandb.init(\n",
    "    # project name\n",
    "    project=\"Federated Non-Stationary Transformer on KDD dataset\",\n",
    "    # experiment name\n",
    "    name=f\"Federated Non-Stationary Transformer on KDD dataset\",\n",
    "    # Hyperparams\n",
    "    config={\n",
    "        \"dataset\": \"KDD210\",\n",
    "        \"preprocess_type\": \"std\",\n",
    "        \"num_user\": n_users,\n",
    "        \"learning_rate\": LR,\n",
    "        \"global_epochs\": GLOBAL_EPOCHS,\n",
    "        \"local_epochs\": LOCAL_EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"num_gpus\": NUM_GPUS,\n",
    "        \"user_ratio\": USER_RATIO,\n",
    "        \"l2_penalty\": L2_PENALTY,\n",
    "        \"total_time_series\": 210,\n",
    "        \"detrending_data\": \"No\"\n",
    "    })\n",
    "\n",
    "config_wanb = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f6ea4457340>\n",
      "Epoch: 1 cost time: 125.74700236320496\n",
      "Epoch: 2 cost time: 143.6630027294159\n",
      "Epoch: 3 cost time: 151.66661739349365\n",
      "Epoch: 4 cost time: 154.90364527702332\n",
      "Epoch: 5 cost time: 163.54127597808838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   3%|         | 1/30 [13:56<6:44:06, 836.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f6ea44dea70>\n",
      "Epoch: 1 cost time: 164.94005465507507\n",
      "Epoch: 2 cost time: 166.4779760837555\n",
      "Epoch: 3 cost time: 164.68456959724426\n",
      "Epoch: 4 cost time: 164.82093024253845\n",
      "Epoch: 5 cost time: 163.94800877571106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   7%|         | 2/30 [29:16<6:53:20, 885.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f6ea4457340>\n",
      "Epoch: 1 cost time: 165.07892489433289\n",
      "Epoch: 2 cost time: 163.1369264125824\n",
      "Epoch: 3 cost time: 163.8198435306549\n",
      "Epoch: 4 cost time: 164.33698296546936\n",
      "Epoch: 5 cost time: 163.02144289016724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  10%|         | 3/30 [44:31<6:44:40, 899.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f6ea440e830>\n",
      "Epoch: 1 cost time: 163.07272958755493\n",
      "Epoch: 2 cost time: 164.27651524543762\n",
      "Epoch: 3 cost time: 165.13199520111084\n",
      "Epoch: 4 cost time: 163.6859233379364\n",
      "Epoch: 5 cost time: 163.88509488105774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  13%|        | 4/30 [59:47<6:32:30, 905.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f6ea4457340>\n",
      "Epoch: 1 cost time: 163.92414259910583\n",
      "Epoch: 2 cost time: 165.2195656299591\n",
      "Epoch: 3 cost time: 164.48137998580933\n",
      "Epoch: 4 cost time: 163.6445713043213\n",
      "Epoch: 5 cost time: 164.48494005203247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  17%|        | 5/30 [1:15:05<6:19:10, 910.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f6ea440e830>\n",
      "Epoch: 1 cost time: 163.53287887573242\n",
      "Epoch: 2 cost time: 165.44242000579834\n",
      "Epoch: 3 cost time: 164.11410808563232\n",
      "Epoch: 4 cost time: 163.76038765907288\n",
      "Epoch: 5 cost time: 164.98385214805603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  20%|        | 6/30 [1:30:23<6:05:08, 912.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f6ea5570e50>\n",
      "Epoch: 1 cost time: 163.52195739746094\n",
      "Epoch: 2 cost time: 164.2893991470337\n",
      "Epoch: 3 cost time: 162.28667736053467\n",
      "Epoch: 4 cost time: 165.2240674495697\n",
      "Epoch: 5 cost time: 163.73306941986084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  23%|       | 7/30 [1:45:39<5:50:22, 914.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f6ea5570e50>\n",
      "Epoch: 1 cost time: 164.29731345176697\n",
      "Epoch: 2 cost time: 164.26582980155945\n",
      "Epoch: 3 cost time: 163.54956030845642\n",
      "Epoch: 4 cost time: 164.6807725429535\n",
      "Epoch: 5 cost time: 163.9693021774292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  27%|       | 8/30 [2:00:57<5:35:30, 915.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f6ea5eef910>\n",
      "Epoch: 1 cost time: 165.31054091453552\n",
      "Epoch: 2 cost time: 163.60822916030884\n",
      "Epoch: 3 cost time: 164.51036024093628\n",
      "Epoch: 4 cost time: 163.90955591201782\n",
      "Epoch: 5 cost time: 164.02576160430908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  30%|       | 9/30 [2:16:14<5:20:30, 915.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f6ea4525600>\n",
      "Epoch: 1 cost time: 163.31677222251892\n",
      "Epoch: 2 cost time: 162.9271092414856\n",
      "Epoch: 3 cost time: 163.58493638038635\n",
      "Epoch: 4 cost time: 163.94323325157166\n",
      "Epoch: 5 cost time: 164.07127690315247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  33%|      | 10/30 [2:31:29<5:05:12, 915.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f6ea440ea10>\n",
      "Epoch: 1 cost time: 163.5562083721161\n",
      "Epoch: 2 cost time: 164.61002945899963\n",
      "Epoch: 3 cost time: 164.7081263065338\n",
      "Epoch: 4 cost time: 163.28288960456848\n",
      "Epoch: 5 cost time: 164.8502221107483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  37%|      | 11/30 [2:46:47<4:50:07, 916.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f6ea44dea70>\n",
      "Epoch: 1 cost time: 163.98833560943604\n",
      "Epoch: 2 cost time: 164.9146752357483\n",
      "Epoch: 3 cost time: 164.30911827087402\n",
      "Epoch: 4 cost time: 166.29416728019714\n",
      "Epoch: 5 cost time: 165.00214457511902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  40%|      | 12/30 [3:02:07<4:35:15, 917.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f6ea440ea10>\n",
      "Epoch: 1 cost time: 163.2917308807373\n",
      "Epoch: 2 cost time: 163.84087991714478\n",
      "Epoch: 3 cost time: 164.26447415351868\n",
      "Epoch: 4 cost time: 163.8515374660492\n",
      "Epoch: 5 cost time: 164.1482813358307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  43%|     | 13/30 [3:17:24<4:19:50, 917.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f6ea4525600>\n",
      "Epoch: 1 cost time: 163.80429315567017\n",
      "Epoch: 2 cost time: 163.70309281349182\n",
      "Epoch: 3 cost time: 163.9418807029724\n",
      "Epoch: 4 cost time: 164.2124378681183\n",
      "Epoch: 5 cost time: 164.8254039287567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  47%|     | 14/30 [3:32:40<4:04:30, 916.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f6ea4525600>\n",
      "Epoch: 1 cost time: 163.48696613311768\n",
      "Epoch: 2 cost time: 163.58064818382263\n",
      "Epoch: 3 cost time: 164.2030589580536\n",
      "Epoch: 4 cost time: 163.80828404426575\n",
      "Epoch: 5 cost time: 164.21638655662537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  50%|     | 15/30 [3:47:55<3:49:06, 916.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f6ea4525600>\n",
      "Epoch: 1 cost time: 163.98768258094788\n",
      "Epoch: 2 cost time: 164.3414158821106\n",
      "Epoch: 3 cost time: 163.40682244300842\n",
      "Epoch: 4 cost time: 164.60830450057983\n",
      "Epoch: 5 cost time: 163.05235075950623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  53%|    | 16/30 [4:03:11<3:33:48, 916.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f6ea5eef910>\n",
      "Epoch: 1 cost time: 165.1165475845337\n",
      "Epoch: 2 cost time: 162.99091291427612\n",
      "Epoch: 3 cost time: 164.49565768241882\n",
      "Epoch: 4 cost time: 164.04292058944702\n",
      "Epoch: 5 cost time: 165.9095597267151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  57%|    | 17/30 [4:18:31<3:18:45, 917.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f6ea5eefa30>\n",
      "Epoch: 1 cost time: 164.7364604473114\n",
      "Epoch: 2 cost time: 164.88936257362366\n",
      "Epoch: 3 cost time: 163.45416283607483\n",
      "Epoch: 4 cost time: 164.0722131729126\n",
      "Epoch: 5 cost time: 164.80070114135742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  60%|    | 18/30 [4:33:49<3:03:32, 917.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f6ea4457340>\n",
      "Epoch: 1 cost time: 163.10232377052307\n",
      "Epoch: 2 cost time: 164.41811203956604\n",
      "Epoch: 3 cost time: 164.95531582832336\n",
      "Epoch: 4 cost time: 163.9080204963684\n",
      "Epoch: 5 cost time: 163.47119188308716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  63%|   | 19/30 [4:49:05<2:48:06, 916.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f6ea440ea10>\n",
      "Epoch: 1 cost time: 163.6343331336975\n",
      "Epoch: 2 cost time: 163.80726981163025\n",
      "Epoch: 3 cost time: 163.59416341781616\n",
      "Epoch: 4 cost time: 164.19333839416504\n",
      "Epoch: 5 cost time: 163.7377107143402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  67%|   | 20/30 [5:04:20<2:32:45, 916.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f6ea4525600>\n",
      "Epoch: 1 cost time: 164.4461793899536\n",
      "Epoch: 2 cost time: 163.78543424606323\n",
      "Epoch: 3 cost time: 164.28201842308044\n",
      "Epoch: 4 cost time: 164.08263278007507\n",
      "Epoch: 5 cost time: 165.36117815971375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  70%|   | 21/30 [5:19:39<2:17:34, 917.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f6ea440e830>\n",
      "Epoch: 1 cost time: 163.7677218914032\n",
      "Epoch: 2 cost time: 165.6214861869812\n",
      "Epoch: 3 cost time: 164.05169892311096\n",
      "Epoch: 4 cost time: 164.84319925308228\n",
      "Epoch: 5 cost time: 164.2530288696289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  73%|  | 22/30 [5:34:58<2:02:20, 917.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f6ea5eef910>\n",
      "Epoch: 1 cost time: 164.09430527687073\n",
      "Epoch: 2 cost time: 162.51943230628967\n",
      "Epoch: 3 cost time: 162.94436883926392\n",
      "Epoch: 4 cost time: 163.64524912834167\n",
      "Epoch: 5 cost time: 163.7032458782196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  77%|  | 23/30 [5:50:12<1:46:56, 916.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f6ea440e830>\n",
      "Epoch: 1 cost time: 162.86320686340332\n",
      "Epoch: 2 cost time: 163.73353791236877\n",
      "Epoch: 3 cost time: 163.56638979911804\n",
      "Epoch: 4 cost time: 164.36911058425903\n",
      "Epoch: 5 cost time: 163.94857454299927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  80%|  | 24/30 [6:05:25<1:31:33, 915.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f6ea5570e50>\n",
      "Epoch: 1 cost time: 163.64808201789856\n",
      "Epoch: 2 cost time: 163.3726291656494\n",
      "Epoch: 3 cost time: 163.91887044906616\n",
      "Epoch: 4 cost time: 164.7215359210968\n",
      "Epoch: 5 cost time: 163.0756278038025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  83%| | 25/30 [6:20:40<1:16:17, 915.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f6ea5eefa30>\n",
      "Epoch: 1 cost time: 163.49613881111145\n",
      "Epoch: 2 cost time: 164.69118094444275\n",
      "Epoch: 3 cost time: 163.669198513031\n",
      "Epoch: 4 cost time: 164.4022994041443\n",
      "Epoch: 5 cost time: 163.44681692123413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  87%| | 26/30 [6:35:56<1:01:02, 915.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f6ea440ea10>\n",
      "Epoch: 1 cost time: 162.789208650589\n",
      "Epoch: 2 cost time: 163.73699760437012\n",
      "Epoch: 3 cost time: 162.84652185440063\n",
      "Epoch: 4 cost time: 162.2617745399475\n",
      "Epoch: 5 cost time: 164.25943636894226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  90%| | 27/30 [6:51:08<45:43, 914.58s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f6ea5eefa30>\n",
      "Epoch: 1 cost time: 164.40664267539978\n",
      "Epoch: 2 cost time: 164.58784341812134\n",
      "Epoch: 3 cost time: 164.74001836776733\n",
      "Epoch: 4 cost time: 164.22862362861633\n",
      "Epoch: 5 cost time: 164.67456650733948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  93%|| 28/30 [7:06:27<30:31, 915.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f6ea5eef970>\n",
      "Epoch: 1 cost time: 163.78876543045044\n",
      "Epoch: 2 cost time: 163.4000072479248\n",
      "Epoch: 3 cost time: 163.13321828842163\n",
      "Epoch: 4 cost time: 164.3012924194336\n",
      "Epoch: 5 cost time: 163.48534440994263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  97%|| 29/30 [7:21:41<15:15, 915.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f6ea5eef910>\n",
      "Epoch: 1 cost time: 164.70007252693176\n",
      "Epoch: 2 cost time: 163.7963650226593\n",
      "Epoch: 3 cost time: 163.2300682067871\n",
      "Epoch: 4 cost time: 164.23265743255615\n",
      "Epoch: 5 cost time: 164.39813661575317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|| 30/30 [7:36:56<00:00, 913.89s/it]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "from math import sqrt\n",
    "\n",
    "server = ServerNsTransformer(model=server_model, test_loader=server_data_list)\n",
    "\n",
    "user_list = []\n",
    "\n",
    "# Create users\n",
    "for i in range(config_wanb.num_user):\n",
    "    user_i = UserNsTransformer(train_loader=user_data_list[i], model=server_model, user_id=i, local_epochs=config_wanb.local_epochs)\n",
    "    user_list.append(user_i)\n",
    "\n",
    "for _ in tqdm(range(config_wanb.global_epochs), desc=f\"Progress\"):\n",
    "    # Distribute initial model to users\n",
    "    server.distribute_model(user_list)\n",
    "    \n",
    "    # Sub-sample users\n",
    "    sub_user_list = random.sample(user_list, int(config_wanb.user_ratio * config_wanb.num_user))\n",
    "\n",
    "    # Check the sub-sampled user and train model\n",
    "    users_loss = 0.0\n",
    "    for user in sub_user_list:\n",
    "        print(user)\n",
    "        user_loss = user.user_train(args)\n",
    "        users_loss += user_loss\n",
    "    # Aggregate weights on server\n",
    "    server.aggregate_weights(sub_user_list)\n",
    "\n",
    "    # Calulate avg loss on selected users\n",
    "    train_loss =  users_loss / len(sub_user_list)\n",
    "\n",
    "    total_mae = []\n",
    "    total_mse = []\n",
    "    for test_loader in server.test_loader:    \n",
    "        mae, mse, rmse, mape, mspe = server.model_eval(args=args, test_loader=test_loader)\n",
    "        total_mae.append(mae)\n",
    "        total_mse.append(mse)\n",
    "    \n",
    "    wandb.log({\"train_loss\": train_loss, \"mae\": sum(total_mae)/len(total_mae), 'rmse': sqrt(sum(total_mse)/len(total_mse))})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
