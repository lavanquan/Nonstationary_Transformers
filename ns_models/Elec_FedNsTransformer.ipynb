{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/qula0496/quan/Nonstationary_Transformers/ns_models'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/qula0496/quan/Nonstationary_Transformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qula0496/quan/.venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_utils import *\n",
    "from utils.model_utils import *\n",
    "# from utils.koopman_utils import *\n",
    "from modules.serverbase import *\n",
    "from modules.userbase import *\n",
    "from modules.servernsTransformer import *\n",
    "from modules.usernsTranformer import *\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.cuda.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files are ready\n",
      "Data loaded..\n",
      "Data aggregated by hour: (26304, 370)\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset\n",
    "download_elec_dataset()\n",
    "\n",
    "# Clean dataset\n",
    "elec_data = clean_elec()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "Std_normalization = 1\n",
    "if Std_normalization:\n",
    "    scaler = StandardScaler()\n",
    "    temp = scaler.fit_transform(elec_data)\n",
    "    norm_means = scaler.mean_\n",
    "    norm_std = scaler.scale_\n",
    "else:\n",
    "    scaler = MinMaxScaler()\n",
    "    temp = scaler.fit_transform(elec_data)\n",
    "\n",
    "elec_data = pd.DataFrame(elec_data, index=elec_data.index, columns = elec_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prompt: write pandas to csv\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# # Save the DataFrame to a CSV file\n",
    "# elec_data.to_csv('/home/qula0496/quan/Nonstationary_Transformers/dataset/electricity/electricity.csv', index_label='date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'electricity.csv'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ns_models.ns_TransformerConfig import NS_TransformerConfig\n",
    "\n",
    "args = NS_TransformerConfig()\n",
    "args.devices = '2'\n",
    "args.data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from data_provider.data_factory import *\n",
    "from data_provider.data_loader import *\n",
    "\n",
    "test_data, test_loader = data_provider(args, flag='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 371)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = 10\n",
    "user_num_ts = int(elec_data.shape[1] / n_users)\n",
    "user_data_list = []\n",
    "server_data_list = []\n",
    "for i in range(n_users):\n",
    "    train_data, train_loader = data_provider(args, flag='train', start=i*user_num_ts+1, end=min(user_num_ts*(i+1)+1, elec_data.shape[1]+1))\n",
    "    test_data, test_loader = data_provider(args, flag='test', start=i*user_num_ts+1, end=min(user_num_ts*(i+1)+1, elec_data.shape[1]+1))\n",
    "    # user_data = train_set.filter(lambda e, idx: idx>=(i*user_num_ts) and idx < user_num_ts*(i+1), with_indices=True)\n",
    "    user_data_list.append(train_loader)\n",
    "    server_data_list.append(test_loader)\n",
    "    # print(train_data[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (enc_embedding): DataEmbedding(\n",
       "    (value_embedding): TokenEmbedding(\n",
       "      (tokenConv): Conv1d(37, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)\n",
       "    )\n",
       "    (position_embedding): PositionalEmbedding()\n",
       "    (temporal_embedding): TimeFeatureEmbedding(\n",
       "      (embed): Linear(in_features=4, out_features=64, bias=False)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (dec_embedding): DataEmbedding(\n",
       "    (value_embedding): TokenEmbedding(\n",
       "      (tokenConv): Conv1d(37, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)\n",
       "    )\n",
       "    (position_embedding): PositionalEmbedding()\n",
       "    (temporal_embedding): TimeFeatureEmbedding(\n",
       "      (embed): Linear(in_features=4, out_features=64, bias=False)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): Encoder(\n",
       "    (attn_layers): ModuleList(\n",
       "      (0-5): 6 x EncoderLayer(\n",
       "        (attention): AttentionLayer(\n",
       "          (inner_attention): DSAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (key_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (value_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (out_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (conv1): Conv1d(64, 32, kernel_size=(1,), stride=(1,))\n",
       "        (conv2): Conv1d(32, 64, kernel_size=(1,), stride=(1,))\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x DecoderLayer(\n",
       "        (self_attention): AttentionLayer(\n",
       "          (inner_attention): DSAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (key_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (value_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (out_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (cross_attention): AttentionLayer(\n",
       "          (inner_attention): DSAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (key_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (value_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (out_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (conv1): Conv1d(64, 32, kernel_size=(1,), stride=(1,))\n",
       "        (conv2): Conv1d(32, 64, kernel_size=(1,), stride=(1,))\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (projection): Linear(in_features=64, out_features=37, bias=True)\n",
       "  )\n",
       "  (tau_learner): Projector(\n",
       "    (series_conv): Conv1d(96, 1, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)\n",
       "    (backbone): Sequential(\n",
       "      (0): Linear(in_features=74, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=1, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (delta_learner): Projector(\n",
       "    (series_conv): Conv1d(96, 1, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)\n",
       "    (backbone): Sequential(\n",
       "      (0): Linear(in_features=74, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=96, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ns_models import ns_Transformer\n",
    "server_model = ns_Transformer.Model(configs=args)\n",
    "server_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup WANDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkenlvq\u001b[0m (\u001b[33mquanla\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb2b9333a221418cb955365c66df46da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112917142195835, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/qula0496/quan/Nonstationary_Transformers/wandb/run-20240609_231734-2tqcc7ry</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/quanla/Federated%20Non-Stationary%20Transformer%20on%20Elec%20dataset/runs/2tqcc7ry' target=\"_blank\">Federated Non-Stationary Transformer on Elec dataset</a></strong> to <a href='https://wandb.ai/quanla/Federated%20Non-Stationary%20Transformer%20on%20Elec%20dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/quanla/Federated%20Non-Stationary%20Transformer%20on%20Elec%20dataset' target=\"_blank\">https://wandb.ai/quanla/Federated%20Non-Stationary%20Transformer%20on%20Elec%20dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/quanla/Federated%20Non-Stationary%20Transformer%20on%20Elec%20dataset/runs/2tqcc7ry' target=\"_blank\">https://wandb.ai/quanla/Federated%20Non-Stationary%20Transformer%20on%20Elec%20dataset/runs/2tqcc7ry</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "import torch\n",
    "\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "LR = 1e-3\n",
    "GLOBAL_EPOCHS = 30\n",
    "LOCAL_EPOCHS = 5\n",
    "BATCH_SIZE = 128\n",
    "L2_PENALTY = 0.0\n",
    "USER_RATIO = 0.1\n",
    "run = wandb.init(\n",
    "    # project name\n",
    "    project=\"Federated Non-Stationary Transformer on Elec dataset\",\n",
    "    # experiment name\n",
    "    name=f\"Federated Non-Stationary Transformer on Elec dataset\",\n",
    "    # Hyperparams\n",
    "    config={\n",
    "        \"dataset\": \"Elec370\",\n",
    "        \"preprocess_type\": \"std\",\n",
    "        \"num_user\": n_users,\n",
    "        \"learning_rate\": LR,\n",
    "        \"global_epochs\": GLOBAL_EPOCHS,\n",
    "        \"local_epochs\": LOCAL_EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"num_gpus\": NUM_GPUS,\n",
    "        \"user_ratio\": USER_RATIO,\n",
    "        \"l2_penalty\": L2_PENALTY,\n",
    "        \"total_time_series\": 370,\n",
    "        \"detrending_data\": \"No\"\n",
    "    })\n",
    "\n",
    "config_wanb = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f5d061cfdc0>\n",
      "Epoch: 1 cost time: 48.093111753463745\n",
      "Epoch: 2 cost time: 48.807565212249756\n",
      "Epoch: 3 cost time: 49.39934682846069\n",
      "Epoch: 4 cost time: 46.57547473907471\n",
      "Epoch: 5 cost time: 50.36606431007385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   3%|▎         | 1/30 [04:11<2:01:24, 251.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f5d0662acb0>\n",
      "Epoch: 1 cost time: 49.40468454360962\n",
      "Epoch: 2 cost time: 49.334137201309204\n",
      "Epoch: 3 cost time: 48.574421644210815\n",
      "Epoch: 4 cost time: 48.49938440322876\n",
      "Epoch: 5 cost time: 47.60334300994873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   7%|▋         | 2/30 [08:22<1:57:17, 251.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f5d02b347f0>\n",
      "Epoch: 1 cost time: 51.20272970199585\n",
      "Epoch: 2 cost time: 51.93982410430908\n",
      "Epoch: 3 cost time: 50.058637857437134\n",
      "Epoch: 4 cost time: 49.64298367500305\n",
      "Epoch: 5 cost time: 49.91506481170654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  10%|█         | 3/30 [12:43<1:55:02, 255.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f5d02b345b0>\n",
      "Epoch: 1 cost time: 49.180177211761475\n",
      "Epoch: 2 cost time: 50.21417570114136\n",
      "Epoch: 3 cost time: 50.41842746734619\n",
      "Epoch: 4 cost time: 50.105724811553955\n",
      "Epoch: 5 cost time: 48.416014194488525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  13%|█▎        | 4/30 [16:59<1:50:50, 255.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f5d02b345b0>\n",
      "Epoch: 1 cost time: 49.951019048690796\n",
      "Epoch: 2 cost time: 50.02640891075134\n",
      "Epoch: 3 cost time: 49.70701551437378\n",
      "Epoch: 4 cost time: 50.86823630332947\n",
      "Epoch: 5 cost time: 49.007816791534424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  17%|█▋        | 5/30 [21:16<1:46:49, 256.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f5d02b345b0>\n",
      "Epoch: 1 cost time: 49.479350328445435\n",
      "Epoch: 2 cost time: 48.16417145729065\n",
      "Epoch: 3 cost time: 49.38860011100769\n",
      "Epoch: 4 cost time: 51.2889883518219\n",
      "Epoch: 5 cost time: 48.682446241378784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  20%|██        | 6/30 [25:30<1:42:10, 255.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f5d02b345b0>\n",
      "Epoch: 1 cost time: 49.00692439079285\n",
      "Epoch: 2 cost time: 50.43352174758911\n",
      "Epoch: 3 cost time: 51.862762689590454\n",
      "Epoch: 4 cost time: 49.645007610321045\n",
      "Epoch: 5 cost time: 48.91507029533386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  23%|██▎       | 7/30 [29:48<1:38:11, 256.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f5d0662b190>\n",
      "Epoch: 1 cost time: 49.60818672180176\n",
      "Epoch: 2 cost time: 50.782845973968506\n",
      "Epoch: 3 cost time: 48.18422818183899\n",
      "Epoch: 4 cost time: 48.78522539138794\n",
      "Epoch: 5 cost time: 48.90997624397278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  27%|██▋       | 8/30 [34:02<1:33:40, 255.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f5d02b363e0>\n",
      "Epoch: 1 cost time: 48.07557201385498\n",
      "Epoch: 2 cost time: 47.82866978645325\n",
      "Epoch: 3 cost time: 50.7519736289978\n",
      "Epoch: 4 cost time: 49.47807598114014\n",
      "Epoch: 5 cost time: 49.0742301940918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  30%|███       | 9/30 [38:15<1:29:09, 254.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f5d0662b190>\n",
      "Epoch: 1 cost time: 48.53178858757019\n",
      "Epoch: 2 cost time: 50.6532518863678\n",
      "Epoch: 3 cost time: 48.043225049972534\n",
      "Epoch: 4 cost time: 49.89193320274353\n",
      "Epoch: 5 cost time: 50.29182481765747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  33%|███▎      | 10/30 [42:30<1:24:58, 254.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f5d0662ad70>\n",
      "Epoch: 1 cost time: 50.11165499687195\n",
      "Epoch: 2 cost time: 49.02253270149231\n",
      "Epoch: 3 cost time: 49.05331254005432\n",
      "Epoch: 4 cost time: 50.08337926864624\n",
      "Epoch: 5 cost time: 48.86036539077759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  37%|███▋      | 11/30 [46:45<1:20:43, 254.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f5d0662b190>\n",
      "Epoch: 1 cost time: 51.061102628707886\n",
      "Epoch: 2 cost time: 49.07788562774658\n",
      "Epoch: 3 cost time: 49.46939563751221\n",
      "Epoch: 4 cost time: 50.98304343223572\n",
      "Epoch: 5 cost time: 49.99218225479126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  40%|████      | 12/30 [51:03<1:16:46, 255.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f5d02b345b0>\n",
      "Epoch: 1 cost time: 49.35778760910034\n",
      "Epoch: 2 cost time: 50.30490326881409\n",
      "Epoch: 3 cost time: 50.24275350570679\n",
      "Epoch: 4 cost time: 50.67565560340881\n",
      "Epoch: 5 cost time: 49.56449055671692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  43%|████▎     | 13/30 [55:21<1:12:42, 256.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f5d02b347f0>\n",
      "Epoch: 1 cost time: 50.16813564300537\n",
      "Epoch: 2 cost time: 50.32768535614014\n",
      "Epoch: 3 cost time: 50.30887198448181\n",
      "Epoch: 4 cost time: 49.03752517700195\n",
      "Epoch: 5 cost time: 51.10018754005432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  47%|████▋     | 14/30 [59:40<1:08:37, 257.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f5d02b363e0>\n",
      "Epoch: 1 cost time: 48.93211507797241\n",
      "Epoch: 2 cost time: 46.012043952941895\n",
      "Epoch: 3 cost time: 46.93571066856384\n",
      "Epoch: 4 cost time: 51.73224973678589\n",
      "Epoch: 5 cost time: 50.70047736167908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  50%|█████     | 15/30 [1:03:53<1:03:56, 255.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f5d0662ad70>\n",
      "Epoch: 1 cost time: 51.69906711578369\n",
      "Epoch: 2 cost time: 50.04984712600708\n",
      "Epoch: 3 cost time: 49.90300750732422\n",
      "Epoch: 4 cost time: 50.03633975982666\n",
      "Epoch: 5 cost time: 49.40744376182556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  53%|█████▎    | 16/30 [1:08:12<59:56, 256.87s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f5d061cfdc0>\n",
      "Epoch: 1 cost time: 49.07307028770447\n",
      "Epoch: 2 cost time: 50.7784469127655\n",
      "Epoch: 3 cost time: 48.32597589492798\n",
      "Epoch: 4 cost time: 49.23596692085266\n",
      "Epoch: 5 cost time: 50.896082401275635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  57%|█████▋    | 17/30 [1:12:28<55:36, 256.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f5d0662ad70>\n",
      "Epoch: 1 cost time: 49.77940893173218\n",
      "Epoch: 2 cost time: 50.669251680374146\n",
      "Epoch: 3 cost time: 50.51072645187378\n",
      "Epoch: 4 cost time: 49.788331031799316\n",
      "Epoch: 5 cost time: 48.12441372871399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  60%|██████    | 18/30 [1:16:45<51:19, 256.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f5d02b345b0>\n",
      "Epoch: 1 cost time: 49.66982841491699\n",
      "Epoch: 2 cost time: 49.17778158187866\n",
      "Epoch: 3 cost time: 51.550827503204346\n",
      "Epoch: 4 cost time: 49.71964454650879\n",
      "Epoch: 5 cost time: 48.17503476142883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  63%|██████▎   | 19/30 [1:21:00<47:00, 256.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f5d02fe2410>\n",
      "Epoch: 1 cost time: 50.673165798187256\n",
      "Epoch: 2 cost time: 49.99402856826782\n",
      "Epoch: 3 cost time: 50.073822021484375\n",
      "Epoch: 4 cost time: 48.55658507347107\n",
      "Epoch: 5 cost time: 48.529035329818726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  67%|██████▋   | 20/30 [1:25:16<42:41, 256.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f5d0662b190>\n",
      "Epoch: 1 cost time: 46.427536964416504\n",
      "Epoch: 2 cost time: 50.923675537109375\n",
      "Epoch: 3 cost time: 49.62218165397644\n",
      "Epoch: 4 cost time: 48.17279624938965\n",
      "Epoch: 5 cost time: 48.52176284790039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  70%|███████   | 21/30 [1:29:27<38:12, 254.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f5d02b347f0>\n",
      "Epoch: 1 cost time: 49.31279683113098\n",
      "Epoch: 2 cost time: 50.81626605987549\n",
      "Epoch: 3 cost time: 51.890260457992554\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "from math import sqrt\n",
    "\n",
    "server = ServerNsTransformer(model=server_model, test_loader=server_data_list)\n",
    "\n",
    "user_list = []\n",
    "\n",
    "# Create users\n",
    "for i in range(config_wanb.num_user):\n",
    "    user_i = UserNsTransformer(train_loader=user_data_list[i], model=server_model, user_id=i, local_epochs=config_wanb.local_epochs)\n",
    "    user_list.append(user_i)\n",
    "\n",
    "for _ in tqdm(range(config_wanb.global_epochs), desc=f\"Progress\"):\n",
    "    # Distribute initial model to users\n",
    "    server.distribute_model(user_list)\n",
    "    \n",
    "    # Sub-sample users\n",
    "    sub_user_list = random.sample(user_list, int(config_wanb.user_ratio * config_wanb.num_user))\n",
    "\n",
    "    # Check the sub-sampled user and train model\n",
    "    users_loss = 0.0\n",
    "    for user in sub_user_list:\n",
    "        print(user)\n",
    "        user_loss = user.user_train(args)\n",
    "        users_loss += user_loss\n",
    "    # Aggregate weights on server\n",
    "    server.aggregate_weights(sub_user_list)\n",
    "\n",
    "    # Calulate avg loss on selected users\n",
    "    train_loss =  users_loss / len(sub_user_list)\n",
    "\n",
    "    total_mae = []\n",
    "    total_mse = []\n",
    "    for test_loader in server.test_loader:    \n",
    "        mae, mse, rmse, mape, mspe = server.model_eval(args=args, test_loader=test_loader)\n",
    "        total_mae.append(mae)\n",
    "        total_mse.append(mse)\n",
    "    \n",
    "    wandb.log({\"train_loss\": train_loss, \"mae\": sum(total_mae)/len(total_mae), 'rmse': sqrt(sum(total_mse)/len(total_mse))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.version\n",
    "\n",
    "torch.version.cuda"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
