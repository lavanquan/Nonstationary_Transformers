{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/qula0496/quan/Nonstationary_Transformers/ns_models'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/qula0496/quan/Nonstationary_Transformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qula0496/quan/.venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_utils import *\n",
    "from utils.model_utils import *\n",
    "# from utils.koopman_utils import *\n",
    "from modules.serverbase import *\n",
    "from modules.userbase import *\n",
    "from modules.servernsTransformer import *\n",
    "from modules.usernsTranformer import *\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.cuda.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['start', 'feat_static_cat', 'feat_dynamic_real', 'item_id', 'target'],\n",
       "        num_rows: 210\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['start', 'feat_static_cat', 'feat_dynamic_real', 'item_id', 'target'],\n",
       "        num_rows: 210\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['start', 'feat_static_cat', 'feat_dynamic_real', 'item_id', 'target'],\n",
       "        num_rows: 210\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"LeoTungAnh/kdd210_hourly\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10898"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset[0]['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[row['item_id']] = row['target']\n",
      "/tmp/ipykernel_3589470/2019493983.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['date'] = '2015-01-01 00:00:00'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>T3</th>\n",
       "      <th>T4</th>\n",
       "      <th>T5</th>\n",
       "      <th>T6</th>\n",
       "      <th>T7</th>\n",
       "      <th>T8</th>\n",
       "      <th>T9</th>\n",
       "      <th>T10</th>\n",
       "      <th>...</th>\n",
       "      <th>T201</th>\n",
       "      <th>T202</th>\n",
       "      <th>T203</th>\n",
       "      <th>T204</th>\n",
       "      <th>T205</th>\n",
       "      <th>T206</th>\n",
       "      <th>T207</th>\n",
       "      <th>T208</th>\n",
       "      <th>T209</th>\n",
       "      <th>T210</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01 00:00:00</th>\n",
       "      <td>1.468122</td>\n",
       "      <td>1.538769</td>\n",
       "      <td>-0.030617</td>\n",
       "      <td>-0.781501</td>\n",
       "      <td>-0.802695</td>\n",
       "      <td>-0.772417</td>\n",
       "      <td>-0.378809</td>\n",
       "      <td>-0.237514</td>\n",
       "      <td>-0.393948</td>\n",
       "      <td>-0.810264</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.131542</td>\n",
       "      <td>-0.785538</td>\n",
       "      <td>-0.807741</td>\n",
       "      <td>-0.777464</td>\n",
       "      <td>1.493353</td>\n",
       "      <td>1.654833</td>\n",
       "      <td>-0.025571</td>\n",
       "      <td>-0.776959</td>\n",
       "      <td>-0.777464</td>\n",
       "      <td>-0.792602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 00:00:00</th>\n",
       "      <td>1.316855</td>\n",
       "      <td>1.449373</td>\n",
       "      <td>-0.079674</td>\n",
       "      <td>-0.773862</td>\n",
       "      <td>-0.798327</td>\n",
       "      <td>-0.767746</td>\n",
       "      <td>-0.512904</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.507808</td>\n",
       "      <td>-0.803933</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.140836</td>\n",
       "      <td>-0.775391</td>\n",
       "      <td>-0.798327</td>\n",
       "      <td>-0.772842</td>\n",
       "      <td>1.444276</td>\n",
       "      <td>1.724601</td>\n",
       "      <td>-0.023609</td>\n",
       "      <td>-0.804443</td>\n",
       "      <td>-0.767746</td>\n",
       "      <td>-0.783036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 00:00:00</th>\n",
       "      <td>1.261700</td>\n",
       "      <td>1.636722</td>\n",
       "      <td>-0.061296</td>\n",
       "      <td>-0.759776</td>\n",
       "      <td>-0.780089</td>\n",
       "      <td>-0.754046</td>\n",
       "      <td>-0.639456</td>\n",
       "      <td>-0.582161</td>\n",
       "      <td>-0.597787</td>\n",
       "      <td>-0.792069</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.134217</td>\n",
       "      <td>-0.759776</td>\n",
       "      <td>-0.780089</td>\n",
       "      <td>-0.754046</td>\n",
       "      <td>1.501297</td>\n",
       "      <td>1.772147</td>\n",
       "      <td>-0.009210</td>\n",
       "      <td>-0.791548</td>\n",
       "      <td>-0.748838</td>\n",
       "      <td>-0.769672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 00:00:00</th>\n",
       "      <td>1.487590</td>\n",
       "      <td>1.831615</td>\n",
       "      <td>-0.022891</td>\n",
       "      <td>-0.730291</td>\n",
       "      <td>-0.753942</td>\n",
       "      <td>-0.721690</td>\n",
       "      <td>-0.646435</td>\n",
       "      <td>-0.560429</td>\n",
       "      <td>-0.555053</td>\n",
       "      <td>-0.765768</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.114272</td>\n",
       "      <td>-0.733516</td>\n",
       "      <td>-0.753942</td>\n",
       "      <td>-0.732441</td>\n",
       "      <td>0.525398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.270158</td>\n",
       "      <td>-0.744267</td>\n",
       "      <td>-0.678687</td>\n",
       "      <td>-0.748567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 00:00:00</th>\n",
       "      <td>1.842409</td>\n",
       "      <td>2.225288</td>\n",
       "      <td>0.150884</td>\n",
       "      <td>-0.702879</td>\n",
       "      <td>-0.723452</td>\n",
       "      <td>-0.694879</td>\n",
       "      <td>-0.632018</td>\n",
       "      <td>-0.500582</td>\n",
       "      <td>-0.454865</td>\n",
       "      <td>-0.741738</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066271</td>\n",
       "      <td>-0.707451</td>\n",
       "      <td>-0.729166</td>\n",
       "      <td>-0.717737</td>\n",
       "      <td>-0.672020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.569157</td>\n",
       "      <td>-0.743453</td>\n",
       "      <td>-0.557728</td>\n",
       "      <td>-0.723452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  210 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           T1        T2        T3        T4        T5  \\\n",
       "date                                                                    \n",
       "2015-01-01 00:00:00  1.468122  1.538769 -0.030617 -0.781501 -0.802695   \n",
       "2015-01-01 00:00:00  1.316855  1.449373 -0.079674 -0.773862 -0.798327   \n",
       "2015-01-01 00:00:00  1.261700  1.636722 -0.061296 -0.759776 -0.780089   \n",
       "2015-01-01 00:00:00  1.487590  1.831615 -0.022891 -0.730291 -0.753942   \n",
       "2015-01-01 00:00:00  1.842409  2.225288  0.150884 -0.702879 -0.723452   \n",
       "\n",
       "                           T6        T7        T8        T9       T10  ...  \\\n",
       "date                                                                   ...   \n",
       "2015-01-01 00:00:00 -0.772417 -0.378809 -0.237514 -0.393948 -0.810264  ...   \n",
       "2015-01-01 00:00:00 -0.767746 -0.512904  0.000000 -0.507808 -0.803933  ...   \n",
       "2015-01-01 00:00:00 -0.754046 -0.639456 -0.582161 -0.597787 -0.792069  ...   \n",
       "2015-01-01 00:00:00 -0.721690 -0.646435 -0.560429 -0.555053 -0.765768  ...   \n",
       "2015-01-01 00:00:00 -0.694879 -0.632018 -0.500582 -0.454865 -0.741738  ...   \n",
       "\n",
       "                         T201      T202      T203      T204      T205  \\\n",
       "date                                                                    \n",
       "2015-01-01 00:00:00 -0.131542 -0.785538 -0.807741 -0.777464  1.493353   \n",
       "2015-01-01 00:00:00 -0.140836 -0.775391 -0.798327 -0.772842  1.444276   \n",
       "2015-01-01 00:00:00 -0.134217 -0.759776 -0.780089 -0.754046  1.501297   \n",
       "2015-01-01 00:00:00 -0.114272 -0.733516 -0.753942 -0.732441  0.525398   \n",
       "2015-01-01 00:00:00 -0.066271 -0.707451 -0.729166 -0.717737 -0.672020   \n",
       "\n",
       "                         T206      T207      T208      T209      T210  \n",
       "date                                                                   \n",
       "2015-01-01 00:00:00  1.654833 -0.025571 -0.776959 -0.777464 -0.792602  \n",
       "2015-01-01 00:00:00  1.724601 -0.023609 -0.804443 -0.767746 -0.783036  \n",
       "2015-01-01 00:00:00  1.772147 -0.009210 -0.791548 -0.748838 -0.769672  \n",
       "2015-01-01 00:00:00  0.000000 -0.270158 -0.744267 -0.678687 -0.748567  \n",
       "2015-01-01 00:00:00  0.000000 -0.569157 -0.743453 -0.557728 -0.723452  \n",
       "\n",
       "[5 rows x 210 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a list of dictionaries, where each dictionary represents a row in the DataFrame\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for row in test_dataset:\n",
    "  df[row['item_id']] = row['target']\n",
    "\n",
    "df['date'] = '2015-01-01 00:00:00'\n",
    "df = df.set_index(\"date\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the DataFrame to a CSV file\n",
    "df.to_csv('/home/qula0496/quan/Nonstationary_Transformers/dataset/kdd/kdd.csv', index_label='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10898, 210)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "id_gpu = '2'\n",
    "n_users = 10\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = id_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kdd.csv'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ns_models.ns_TransformerConfig import NS_TransformerConfig\n",
    "user_num_ts = int(df.shape[1] / n_users)\n",
    "args = NS_TransformerConfig()\n",
    "args.devices = id_gpu\n",
    "args.enc_in = user_num_ts\n",
    "args.dec_in = user_num_ts\n",
    "args.c_out = user_num_ts\n",
    "args.root_path = './dataset/kdd/'\n",
    "args.data_path = 'kdd.csv'\n",
    "args.model_id = 'kdd_96_96'\n",
    "args.data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from data_provider.data_factory import *\n",
    "from data_provider.data_loader import *\n",
    "\n",
    "test_data, test_loader = data_provider(args, flag='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data_list = []\n",
    "server_data_list = []\n",
    "for i in range(n_users):\n",
    "    train_data, train_loader = data_provider(args, flag='train', start=i*user_num_ts+1, end=min(user_num_ts*(i+1)+1, df.shape[1]+1))\n",
    "    test_data, test_loader = data_provider(args, flag='test', start=i*user_num_ts+1, end=min(user_num_ts*(i+1)+1, df.shape[1]+1))\n",
    "    # user_data = train_set.filter(lambda e, idx: idx>=(i*user_num_ts) and idx < user_num_ts*(i+1), with_indices=True)\n",
    "    user_data_list.append(train_loader)\n",
    "    server_data_list.append(test_loader)\n",
    "    # print(train_data[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (enc_embedding): DataEmbedding(\n",
       "    (value_embedding): TokenEmbedding(\n",
       "      (tokenConv): Conv1d(21, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)\n",
       "    )\n",
       "    (position_embedding): PositionalEmbedding()\n",
       "    (temporal_embedding): TimeFeatureEmbedding(\n",
       "      (embed): Linear(in_features=4, out_features=64, bias=False)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (dec_embedding): DataEmbedding(\n",
       "    (value_embedding): TokenEmbedding(\n",
       "      (tokenConv): Conv1d(21, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)\n",
       "    )\n",
       "    (position_embedding): PositionalEmbedding()\n",
       "    (temporal_embedding): TimeFeatureEmbedding(\n",
       "      (embed): Linear(in_features=4, out_features=64, bias=False)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): Encoder(\n",
       "    (attn_layers): ModuleList(\n",
       "      (0-5): 6 x EncoderLayer(\n",
       "        (attention): AttentionLayer(\n",
       "          (inner_attention): DSAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (key_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (value_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (out_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (conv1): Conv1d(64, 32, kernel_size=(1,), stride=(1,))\n",
       "        (conv2): Conv1d(32, 64, kernel_size=(1,), stride=(1,))\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x DecoderLayer(\n",
       "        (self_attention): AttentionLayer(\n",
       "          (inner_attention): DSAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (key_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (value_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (out_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (cross_attention): AttentionLayer(\n",
       "          (inner_attention): DSAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (key_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (value_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (out_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (conv1): Conv1d(64, 32, kernel_size=(1,), stride=(1,))\n",
       "        (conv2): Conv1d(32, 64, kernel_size=(1,), stride=(1,))\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (projection): Linear(in_features=64, out_features=21, bias=True)\n",
       "  )\n",
       "  (tau_learner): Projector(\n",
       "    (series_conv): Conv1d(96, 1, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)\n",
       "    (backbone): Sequential(\n",
       "      (0): Linear(in_features=42, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=1, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (delta_learner): Projector(\n",
       "    (series_conv): Conv1d(96, 1, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)\n",
       "    (backbone): Sequential(\n",
       "      (0): Linear(in_features=42, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=96, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ns_models import ns_Transformer\n",
    "server_model = ns_Transformer.Model(configs=args)\n",
    "server_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:oe73g8be) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4fc820b087f4fb1b64eaa3fc66d018e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.009 MB uploaded\\r'), FloatProgress(value=0.5110575411382454, max=1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Federated Non-Stationary Transformer on KDD dataset</strong> at: <a href='https://wandb.ai/quanla/Federated%20Non-Stationary%20Transformer%20on%20KDD%20dataset/runs/oe73g8be' target=\"_blank\">https://wandb.ai/quanla/Federated%20Non-Stationary%20Transformer%20on%20KDD%20dataset/runs/oe73g8be</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240610_160543-oe73g8be/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:oe73g8be). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ef77f483d134c64bf65ddf3c51e2203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113121215667989, max=1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/qula0496/quan/Nonstationary_Transformers/wandb/run-20240610_160639-qx9zcowq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/quanla/Federated%20Non-Stationary%20Transformer%20on%20KDD%20dataset/runs/qx9zcowq' target=\"_blank\">Federated Non-Stationary Transformer on KDD dataset</a></strong> to <a href='https://wandb.ai/quanla/Federated%20Non-Stationary%20Transformer%20on%20KDD%20dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/quanla/Federated%20Non-Stationary%20Transformer%20on%20KDD%20dataset' target=\"_blank\">https://wandb.ai/quanla/Federated%20Non-Stationary%20Transformer%20on%20KDD%20dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/quanla/Federated%20Non-Stationary%20Transformer%20on%20KDD%20dataset/runs/qx9zcowq' target=\"_blank\">https://wandb.ai/quanla/Federated%20Non-Stationary%20Transformer%20on%20KDD%20dataset/runs/qx9zcowq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "import torch\n",
    "\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "LR = 1e-3\n",
    "GLOBAL_EPOCHS = 30\n",
    "LOCAL_EPOCHS = 5\n",
    "BATCH_SIZE = 128\n",
    "L2_PENALTY = 0.0\n",
    "USER_RATIO = 0.1\n",
    "run = wandb.init(\n",
    "    # project name\n",
    "    project=\"Federated Non-Stationary Transformer on KDD dataset\",\n",
    "    # experiment name\n",
    "    name=f\"Federated Non-Stationary Transformer on KDD dataset\",\n",
    "    # Hyperparams\n",
    "    config={\n",
    "        \"dataset\": \"KDD210\",\n",
    "        \"preprocess_type\": \"std\",\n",
    "        \"num_user\": n_users,\n",
    "        \"learning_rate\": LR,\n",
    "        \"global_epochs\": GLOBAL_EPOCHS,\n",
    "        \"local_epochs\": LOCAL_EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"num_gpus\": NUM_GPUS,\n",
    "        \"user_ratio\": USER_RATIO,\n",
    "        \"l2_penalty\": L2_PENALTY,\n",
    "        \"total_time_series\": 210,\n",
    "        \"detrending_data\": \"No\"\n",
    "    })\n",
    "\n",
    "config_wanb = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f27c0358580>\n",
      "Epoch: 1 cost time: 21.603606462478638\n",
      "Epoch: 2 cost time: 20.92341160774231\n",
      "Epoch: 3 cost time: 20.81333041191101\n",
      "Epoch: 4 cost time: 21.61803102493286\n",
      "Epoch: 5 cost time: 21.049418926239014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   3%|         | 1/30 [01:52<54:28, 112.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f27a5dd9cc0>\n",
      "Epoch: 1 cost time: 21.389975786209106\n",
      "Epoch: 2 cost time: 20.545129537582397\n",
      "Epoch: 3 cost time: 20.940517902374268\n",
      "Epoch: 4 cost time: 21.306363344192505\n",
      "Epoch: 5 cost time: 21.624013423919678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   7%|         | 2/30 [03:45<52:33, 112.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f27a430abc0>\n",
      "Epoch: 1 cost time: 21.663957834243774\n",
      "Epoch: 2 cost time: 21.00654172897339\n",
      "Epoch: 3 cost time: 20.907637357711792\n",
      "Epoch: 4 cost time: 20.88413906097412\n",
      "Epoch: 5 cost time: 20.983036518096924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  10%|         | 3/30 [05:36<50:25, 112.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f27a5e625f0>\n",
      "Epoch: 1 cost time: 20.87943148612976\n",
      "Epoch: 2 cost time: 21.989014863967896\n",
      "Epoch: 3 cost time: 21.887808084487915\n",
      "Epoch: 4 cost time: 21.652119398117065\n",
      "Epoch: 5 cost time: 22.072620630264282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  13%|        | 4/30 [07:32<49:08, 113.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f27a5e60e20>\n",
      "Epoch: 1 cost time: 21.68179178237915\n",
      "Epoch: 2 cost time: 22.299766302108765\n",
      "Epoch: 3 cost time: 21.18703031539917\n",
      "Epoch: 4 cost time: 21.61335849761963\n",
      "Epoch: 5 cost time: 22.023741722106934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  17%|        | 5/30 [09:27<47:35, 114.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f27a4308b50>\n",
      "Epoch: 1 cost time: 21.376240491867065\n",
      "Epoch: 2 cost time: 20.884332418441772\n",
      "Epoch: 3 cost time: 21.72653031349182\n",
      "Epoch: 4 cost time: 21.82810926437378\n",
      "Epoch: 5 cost time: 20.838860511779785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  20%|        | 6/30 [11:21<45:38, 114.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f27a5dd9cc0>\n",
      "Epoch: 1 cost time: 21.234533071517944\n",
      "Epoch: 2 cost time: 21.63773775100708\n",
      "Epoch: 3 cost time: 21.918954610824585\n",
      "Epoch: 4 cost time: 22.104033708572388\n",
      "Epoch: 5 cost time: 20.877275705337524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  23%|       | 7/30 [13:16<43:46, 114.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f27c0358730>\n",
      "Epoch: 1 cost time: 21.67485809326172\n",
      "Epoch: 2 cost time: 22.052091121673584\n",
      "Epoch: 3 cost time: 22.070603609085083\n",
      "Epoch: 4 cost time: 21.593260049819946\n",
      "Epoch: 5 cost time: 21.267752408981323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  27%|       | 8/30 [15:11<42:03, 114.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f27c0358730>\n",
      "Epoch: 1 cost time: 20.96141815185547\n",
      "Epoch: 2 cost time: 21.437246084213257\n",
      "Epoch: 3 cost time: 21.53958296775818\n",
      "Epoch: 4 cost time: 21.74193572998047\n",
      "Epoch: 5 cost time: 20.867156505584717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  30%|       | 9/30 [17:04<39:54, 114.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f27c0358580>\n",
      "Epoch: 1 cost time: 21.3738694190979\n",
      "Epoch: 2 cost time: 21.566789388656616\n",
      "Epoch: 3 cost time: 21.8889479637146\n",
      "Epoch: 4 cost time: 22.08741784095764\n",
      "Epoch: 5 cost time: 21.5912184715271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  33%|      | 10/30 [18:59<38:08, 114.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f27c03587c0>\n",
      "Epoch: 1 cost time: 21.094717502593994\n",
      "Epoch: 2 cost time: 22.15983009338379\n",
      "Epoch: 3 cost time: 21.93462109565735\n",
      "Epoch: 4 cost time: 21.84104037284851\n",
      "Epoch: 5 cost time: 21.754446506500244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  37%|      | 11/30 [20:55<36:20, 114.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f27c0358a90>\n",
      "Epoch: 1 cost time: 20.856236934661865\n",
      "Epoch: 2 cost time: 21.60232138633728\n",
      "Epoch: 3 cost time: 21.882388830184937\n",
      "Epoch: 4 cost time: 22.314557552337646\n",
      "Epoch: 5 cost time: 21.906540155410767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  40%|      | 12/30 [22:50<34:28, 114.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f27c03587c0>\n",
      "Epoch: 1 cost time: 21.0975558757782\n",
      "Epoch: 2 cost time: 21.79837989807129\n",
      "Epoch: 3 cost time: 21.24030375480652\n",
      "Epoch: 4 cost time: 21.804751873016357\n",
      "Epoch: 5 cost time: 22.064353704452515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  43%|     | 13/30 [24:43<32:25, 114.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f27a5e60e20>\n",
      "Epoch: 1 cost time: 20.644610166549683\n",
      "Epoch: 2 cost time: 21.37205171585083\n",
      "Epoch: 3 cost time: 21.110379934310913\n",
      "Epoch: 4 cost time: 21.41662287712097\n",
      "Epoch: 5 cost time: 21.006375551223755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  47%|     | 14/30 [26:35<30:19, 113.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f27a5e625f0>\n",
      "Epoch: 1 cost time: 20.70546317100525\n",
      "Epoch: 2 cost time: 20.949053525924683\n",
      "Epoch: 3 cost time: 21.299465656280518\n",
      "Epoch: 4 cost time: 22.125174522399902\n",
      "Epoch: 5 cost time: 21.256469249725342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  50%|     | 15/30 [28:28<28:22, 113.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f27a5e625f0>\n",
      "Epoch: 1 cost time: 21.922719955444336\n",
      "Epoch: 2 cost time: 21.79369831085205\n",
      "Epoch: 3 cost time: 21.507594347000122\n",
      "Epoch: 4 cost time: 21.457240104675293\n",
      "Epoch: 5 cost time: 22.063159227371216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  53%|    | 16/30 [30:23<26:35, 113.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f27c03587c0>\n",
      "Epoch: 1 cost time: 22.067496299743652\n",
      "Epoch: 2 cost time: 21.478306531906128\n",
      "Epoch: 3 cost time: 21.19416069984436\n",
      "Epoch: 4 cost time: 21.293540239334106\n",
      "Epoch: 5 cost time: 22.110984325408936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  57%|    | 17/30 [32:18<24:45, 114.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.usernsTranformer.UserNsTransformer object at 0x7f27c0358a90>\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "from math import sqrt\n",
    "\n",
    "server = ServerNsTransformer(model=server_model, test_loader=server_data_list)\n",
    "\n",
    "user_list = []\n",
    "\n",
    "# Create users\n",
    "for i in range(config_wanb.num_user):\n",
    "    user_i = UserNsTransformer(train_loader=user_data_list[i], model=server_model, user_id=i, local_epochs=config_wanb.local_epochs)\n",
    "    user_list.append(user_i)\n",
    "\n",
    "for _ in tqdm(range(config_wanb.global_epochs), desc=f\"Progress\"):\n",
    "    # Distribute initial model to users\n",
    "    server.distribute_model(user_list)\n",
    "    \n",
    "    # Sub-sample users\n",
    "    sub_user_list = random.sample(user_list, int(config_wanb.user_ratio * config_wanb.num_user))\n",
    "\n",
    "    # Check the sub-sampled user and train model\n",
    "    users_loss = 0.0\n",
    "    for user in sub_user_list:\n",
    "        print(user)\n",
    "        user_loss = user.user_train(args)\n",
    "        users_loss += user_loss\n",
    "    # Aggregate weights on server\n",
    "    server.aggregate_weights(sub_user_list)\n",
    "\n",
    "    # Calulate avg loss on selected users\n",
    "    train_loss =  users_loss / len(sub_user_list)\n",
    "\n",
    "    total_mae = []\n",
    "    total_mse = []\n",
    "    for test_loader in server.test_loader:    \n",
    "        mae, mse, rmse, mape, mspe = server.model_eval(args=args, test_loader=test_loader)\n",
    "        total_mae.append(mae)\n",
    "        total_mse.append(mse)\n",
    "    \n",
    "    wandb.log({\"train_loss\": train_loss, \"mae\": sum(total_mae)/len(total_mae), 'rmse': sqrt(sum(total_mse)/len(total_mse))})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
